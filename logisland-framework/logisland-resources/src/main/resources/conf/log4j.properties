# Root logger option
log4j.rootLogger=DEBUG,stdout,rolling


# Direct log messages to a log file
log4j.appender.rolling=org.apache.log4j.RollingFileAppender
log4j.appender.rolling.DatePattern='.'yyyy-MM-dd
log4j.appender.rolling.layout=org.apache.log4j.PatternLayout
log4j.appender.rolling.layout.conversionPattern=[%d] %p %m (%c)%n
log4j.appender.rolling.maxFileSize=50MB
log4j.appender.rolling.maxBackupIndex=5
log4j.appender.rolling.ImmediateFlush=true
#log4j.appender.rolling.file=${spark.yarn.app.container.log.dir}/spark.log
log4j.appender.rolling.encoding=UTF-8

# Direct log messages to stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.Target=System.out
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n


log4j.logger.kafka=ERROR, stdout
log4j.logger.org.apache.zookeeper=ERROR, stdout
log4j.logger.org.apache.kafka=ERROR, stdout
log4j.logger.org.I0Itec.zkclient=ERROR, stdout
log4j.additivity.kafka.server=false
log4j.additivity.kafka.consumer.ZookeeperConsumerConnector=false


# Log levels
log4j.logger.org.apache.spark=DEBUG
log4j.logger.org.apache.spark.scheduler=DEBUG
log4j.logger.org.apache.spark.history=DEBUG
log4j.logger.org.apache.spark.streaming=DEBUG
log4j.logger.org.spark-project.jetty=DEBUG
log4j.logger.org.eclipse.jetty.server=OFF
log4j.logger.org.apache.spark.deploy.yarn=DEBUG 
log4j.logger.io.netty=DEBUG
log4j.logger.org.apache.hadoop.ipc.Client=DEBUG
log4j.logger.org.apache.hadoop=DEBUG
log4j.logger.org.apache.hadoop.ipc.ProtobufRpcEngine=DEBUG
log4j.logger.parquet.hadoop=DEBUG
log4j.logger.org.elasticsearch=DEBUG

log4j.logger.com.hurence=DEBUG
log4j.logger.org.jinterop=DEBUG

log4j.logger.org.apache.spark.deploy.yarn.Client=DEBUG
